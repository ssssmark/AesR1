# Copyright 2025 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import re
import pathlib
import random
from math import isnan
from datetime import datetime
from dataclasses import dataclass, field
from typing import Optional
from babel.numbers import parse_decimal
from utils.math import compute_score
from datasets import load_dataset, load_from_disk
from transformers import Qwen2VLForConditionalGeneration

from math_verify import parse, verify
from open_r1.trainer import VLMGRPOTrainer, GRPOConfig
from trl import ModelConfig, ScriptArguments, TrlParser, get_peft_config
import PIL
from Levenshtein import ratio
from open_r1.utils.pycocotools.coco import COCO
from open_r1.utils.pycocotools.cocoeval import COCOeval
import json
import math
from json_repair import repair_json

from open_r1.vlm_modules import *

from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VLVisionFlashAttention2, apply_rotary_pos_emb_flashatt, flash_attn_varlen_func
import torch
from typing import Tuple
from transformers.utils import logging
from transformers import AutoProcessor, AutoTokenizer
import numpy as np
from openai import OpenAI
from scipy.stats import spearmanr
import itertools
logger = logging.get_logger(__name__)

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY", "sk-proj-1234567890"),
    base_url=os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
)

from open_r1.qwen2_5vl_monkey_patch import monkey_patch_qwen2_5vl_flash_attn, monkey_patch_qwen2_5vl_forward
monkey_patch_qwen2_5vl_flash_attn()    



tokenizer = None

def initialize_tokenizer(model_path):
    global tokenizer
    if tokenizer is None:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
    return tokenizer

@dataclass
class GRPOScriptArguments(ScriptArguments):
    """
    Script arguments for the GRPO training script.
    """
    data_file_paths: str = field(
        default=None,
        metadata={"help": "Paths to data files, separated by ':'"},
    )
    image_folders: str = field(
        default=None,
        metadata={"help": "Paths to image folders, separated by ':'"},
    )
    arrow_cache_dir: str = field(
        default=None,
        metadata={"help": "Path to arrow cache directory"},
    )
    val_split_ratio: float = field(
        default=0.0,
        metadata={"help": "Ratio of validation split, default 0.0"},
    )
    reward_funcs: list[str] = field(
        default_factory=lambda: ["accuracy", "format"],
        metadata={"help": "List of reward functions. Possible values: 'accuracy', 'format', 'plcc'"},
    )
    max_pixels: Optional[int] = field(
        default=12845056,
        metadata={"help": "Maximum number of pixels for the image (for QwenVL)"},
    )
    min_pixels: Optional[int] = field(
        default=3136,
        metadata={"help": "Minimum number of pixels for the image (for QwenVL)"},
    )
    max_anyres_num: Optional[int] = field(
        default=12,
        metadata={"help": "Maximum number of anyres blocks for the image (for InternVL)"},
    )
    reward_method: Optional[str] = field(
        default=None,
        metadata={
            "help": "Choose reward method: 'default', 'mcp', ..."
        },
    )
    question_template: Optional[str] = field(
        default="scoring",
        metadata={
            "help": "Choose scoring or comparing question"
        },
    )


def extract_first_number(model_answer):
    match = re.search(r'-?\d+(\.\d+)?', model_answer)
    if match:
        return float(match.group())
    else:
        return random.randint(1, 5)


def fidelity_reward(pred1, pred2, var1, var2, gt, device):
    esp = 1e-6
    try:
        normal_dist = torch.distributions.Normal(0, 1)
        _cur = (pred1 - pred2) / torch.sqrt(var1 + var2 + esp)
        p = normal_dist.cdf(_cur)
    except:
        print("Meet Error ...")
        p = torch.tensor(0.5, dtype=torch.float32, device=device)
    
    reward = torch.sqrt(p * gt + esp) + torch.sqrt((1 - p) * (1 - gt) + esp)
    return reward
def error_reward(completions, solution, **kwargs):

    device = kwargs.get("device")
    n_gen = kwargs.get("num_generations")
    eps = kwargs.get("eps", 1e-5)  
    reshaped_solution = [solution[i:i + n_gen] for i in range(0, len(solution), n_gen)]
    for i in range(len(reshaped_solution)):
        for j in range(len(reshaped_solution[i])):
            _cur = reshaped_solution[i][j]
            sol_match = re.search(r'<answer>(.*?)</answer>', _cur)
            ground_truth = sol_match.group(1).strip() if sol_match else sol_match.strip()
            reshaped_solution[i][j] = float(ground_truth)
    
    contents = [completion[0]["content"] for completion in completions]
    reshaped_content = [contents[i:i + n_gen] for i in range(0, len(contents), n_gen)]

    batch_pred = []
    for i in range(len(reshaped_content)):
        cur_pred_list = []
        for j in range(len(reshaped_content[i])):
            try:
                content_matches = re.findall(r'<answer>(.*?)</answer>', reshaped_content[i][j], re.DOTALL)
                student_answer = content_matches[-1].strip() if content_matches else reshaped_content[i][j].strip()
                pred = extract_first_number(student_answer)
            except:
                print("Meet Error ...")
                pred = random.uniform(0, 1)
            cur_pred_list.append(pred)
        batch_pred.append(cur_pred_list)
    
    rewards = []
    for i in range(len(batch_pred)):
        for j in range(len(batch_pred[i])):
            pred = torch.tensor(batch_pred[i][j], dtype=torch.float32, device=device)
            gt = torch.tensor(reshaped_solution[i][j], dtype=torch.float32, device=device)
            diff = torch.abs(pred - gt)   
            reward=torch.exp(-0.5 * ((diff) / 0.1) ** 2)+ eps
            try:
                if math.isnan(reward):
                    raise ValueError("reward is NaN")
            except:
                reward = 0.0
            rewards.append(float(f"{reward:.4f}"))
            
            if os.getenv("DEBUG_MODE") == "true":
                log_path = os.getenv("LOG_PATH")
                current_time = datetime.now().strftime("%d-%H-%M-%S-%f")
                image_path = kwargs.get("image_path") if "image_path" in kwargs else None
                problem = kwargs.get("problem")[0]
                image_path = [image_path[i:i + n_gen] for i in range(0, len(image_path), n_gen)]
                
                with open(log_path.replace(".txt", "_error.txt"), "a", encoding='utf-8') as f:
                    f.write(f"------------- {current_time} Error reward: {reward:.4f} -------------\n")
                    f.write(f"error_reward_method: {reward:.4f}\n")
                    f.write(f"image_path: {image_path[i][j]}\n")
                    f.write(f"problem: {problem}\n")
                    f.write(f"Content: {reshaped_content[i][j]}\n")
                    f.write(f"Solution: {reshaped_solution[i][j]}\n")
                    f.write(f"Prediction: {batch_pred[i][j]}\n")
                    f.write(f"Error: {diff:.4f}\n")
    
    return rewards
def binary_reward(completions, solution, **kwargs):

    device = kwargs.get("device")
    n_gen = kwargs.get("num_generations")
    reshaped_solution = [solution[i:i + n_gen] for i in range(0, len(solution), n_gen)]
    for i in range(len(reshaped_solution)):
        for j in range(len(reshaped_solution[i])):
            _cur = reshaped_solution[i][j]
            sol_match = re.search(r'<answer>(.*?)</answer>', _cur)
            ground_truth = sol_match.group(1).strip() if sol_match else sol_match.strip()
            reshaped_solution[i][j] = float(ground_truth)
    contents = [completion[0]["content"] for completion in completions]
    reshaped_content = [contents[i:i + n_gen] for i in range(0, len(contents), n_gen)]

    batch_pred = []
    for i in range(len(reshaped_content)):
        cur_pred_list = []
        for j in range(len(reshaped_content[i])):
            try:
                content_matches = re.findall(r'<answer>(.*?)</answer>', reshaped_content[i][j], re.DOTALL)
                student_answer = content_matches[-1].strip() if content_matches else reshaped_content[i][j].strip()
                pred = extract_first_number(student_answer)
            except:
                print("Meet Error ...")
                pred = random.uniform(0, 1)
            cur_pred_list.append(pred)
        batch_pred.append(cur_pred_list)
        
    rewards = []
    for i in range(len(batch_pred)):
        for j in range(len(batch_pred[i])):
            pred = torch.tensor(batch_pred[i][j], dtype=torch.float32, device=device)
            gt = torch.tensor(reshaped_solution[i][j], dtype=torch.float32, device=device)
        
            diff = torch.abs(pred - gt)
            if diff< 0.07:
                reward = 1
            elif diff < 0.1:
                reward = 0.5
            else:
                reward=0   
            try:
                if math.isnan(reward):
                    raise ValueError("reward is NaN")
            except:
                reward = 0.0
            rewards.append(float(f"{reward:.4f}"))
            
            # 调试日志
            if os.getenv("DEBUG_MODE") == "true":
                log_path = os.getenv("LOG_PATH")
                current_time = datetime.now().strftime("%d-%H-%M-%S-%f")
                image_path = kwargs.get("image_path") if "image_path" in kwargs else None
                problem = kwargs.get("problem")[0]
                image_path = [image_path[i:i + n_gen] for i in range(0, len(image_path), n_gen)]
                
                with open(log_path.replace(".txt", "_binary.txt"), "a", encoding='utf-8') as f:
                    f.write(f"------------- {current_time} binary reward: {reward:.4f} -------------\n")
                    f.write(f"binary_reward_method: {reward:.4f}\n")
                    f.write(f"image_path: {image_path[i][j]}\n")
                    f.write(f"problem: {problem}\n")
                    f.write(f"Content: {reshaped_content[i][j]}\n")
                    f.write(f"Solution: {reshaped_solution[i][j]}\n")
                    f.write(f"Prediction: {batch_pred[i][j]}\n")
                    f.write(f"Error: {diff:.4f}\n")
    
    return rewards


def rank_reward(completions, solution, **kwargs):
    device = kwargs.get("device")
    n_gen = kwargs.get("num_generations")
    reshaped_solution = [solution[i:i + n_gen] for i in range(0, len(solution), n_gen)]
    for i in range(len(reshaped_solution)):
        for j in range(len(reshaped_solution[i])):
            _cur = reshaped_solution[i][j]
            sol_match = re.search(r'<answer>(.*?)</answer>', _cur)
            ground_truth = sol_match.group(1).strip() if sol_match else sol_match.strip()
            reshaped_solution[i][j] = float(ground_truth)

    contents = [completion[0]["content"] for completion in completions]
    reshaped_content = [contents[i:i + n_gen] for i in range(0, len(contents), n_gen)]

    batch_mean, batch_var, batch_pred = [], [], []
    for i in range(len(reshaped_content)): # batch
        cur_pred_list = []
        for j in range(len(reshaped_content[i])): # num generations
            try:
                content_matches = re.findall(r'<answer>(.*?)</answer>', reshaped_content[i][j], re.DOTALL)
                student_answer = content_matches[-1].strip() if content_matches else reshaped_content[i][j].strip()
                pred = extract_first_number(student_answer)
            except:
                print("Meet Error ...")
                pred = random.uniform(0,1)
            cur_pred_list.append(pred)
        
        batch_pred.append(cur_pred_list)
        p = torch.tensor(cur_pred_list, dtype=torch.float32, device=device)
        p_mean = torch.mean(p)
        p_var = torch.var(p)
        batch_mean.append([p_mean])
        batch_var.append([p_var])
    
    rewards = []
    for i in range(len(batch_pred)):
        for j in range(len(batch_pred[i])):
            _reward_sum, _count_idx = 0, 0
            for z in range(len(batch_mean)):
                if z != i:
                   
                    input_pred1 = batch_pred[i][j]
                    input_pred2 = batch_mean[z][0]
                    input_var1 = batch_var[i][0]
                    input_var2 = batch_var[z][0]

                    if reshaped_solution[i][j] >= reshaped_solution[z][0]:
                        input_gt = torch.tensor(1.0, dtype=torch.float32, device=device)
                    elif reshaped_solution[i][j] < reshaped_solution[z][0]:
                        input_gt = torch.tensor(0.0, dtype=torch.float32, device=device)

                    _reward = fidelity_reward(
                        pred1=input_pred1, pred2=input_pred2, var1=input_var1, 
                        var2=input_var2, gt=input_gt, device=device
                    )

                    _reward_sum = _reward_sum + _reward
                    _count_idx = _count_idx + 1

            _cur_reward = _reward_sum / _count_idx
            _cur_reward =float(f"{_cur_reward:.4f}")
            try:
                if math.isnan(_cur_reward):
                    raise ValueError("reward is NaN")
            except:
                _cur_reward = 0.0
            rewards.append(_cur_reward)

            if os.getenv("DEBUG_MODE") == "true":
                log_path = os.getenv("LOG_PATH")
                current_time = datetime.now().strftime("%d-%H-%M-%S-%f")
                image_path = kwargs.get("image_path") if "image_path" in kwargs else None
                problem = kwargs.get("problem")[0]
                image_path = [image_path[i:i + n_gen] for i in range(0, len(image_path), n_gen)]

                with open(log_path.replace(".txt", "_rank.txt"), "a", encoding='utf-8') as f:
                    f.write(f"------------- {current_time} rank reward: {_cur_reward} -------------\n")
                    f.write(f"rank_reward_method: {_cur_reward}\n")
                    f.write(f"image_path: {image_path[i][j]}\n")
                    f.write(f"problem: {problem}\n")
                    f.write(f"Content: {reshaped_content[i][j]}\n")
                    f.write(f"Solution: {reshaped_solution[i][j]}\n") 
    
    return rewards

 
def format_reward(completions, **kwargs):
    """Reward function that checks if the completion has a specific format."""
    pattern = r"<think>.*?</think>\s*<answer>.*?</answer>"
    completion_contents = [completion[0]["content"] for completion in completions]
    matches = [re.fullmatch(pattern, content, re.DOTALL) for content in completion_contents]

    current_time = datetime.now().strftime("%d-%H-%M-%S-%f")
    if os.getenv("DEBUG_MODE") == "true":
        log_path = os.getenv("LOG_PATH")
        with open(log_path.replace(".txt", "_format.txt"), "a", encoding='utf-8') as f:
            f.write(f"------------- {current_time} Format reward -------------\n")
            for content, match in zip(completion_contents, matches):
                f.write(f"Content: {content}\n")
                f.write(f"Has format: {bool(match)}\n")

    return [1.0 if match else 0.0 for match in matches]



reward_funcs_registry = {
    "rank":rank_reward,
    "error":error_reward,
    "format": format_reward,
    
}

@dataclass
class GRPOModelConfig(ModelConfig):
    freeze_vision_modules: bool = False


def get_vlm_module(model_name_or_path):
    if "qwen" in model_name_or_path.lower():
        return Qwen2VLModule
    elif "internvl" in model_name_or_path.lower():
        return InvernVLModule
    else:
        raise ValueError(f"Unsupported model: {model_name_or_path}")

def main(script_args, training_args, model_args):
    # Load the VLM module
    vlm_module_cls = get_vlm_module(model_args.model_name_or_path)
    print("using vlm module:", vlm_module_cls.__name__)
    question_prompt = vlm_module_cls.get_question_template(task_type=script_args.question_template)

    # Get reward functions
    reward_funcs = [reward_funcs_registry[func] for func in reward_funcs_registry.keys()]
    print("reward_funcs:", reward_funcs)

    # Load the JSONL datasets
    import json
    from datasets import Dataset
    
    data_files = script_args.data_file_paths.split(":")
    image_folders = script_args.image_folders.split(":")
    
    if len(data_files) != len(image_folders):
        raise ValueError("Number of data files must match number of image folders")
    
    if script_args.reward_method is None:
        accu_reward_methods = ["default"] * len(data_files)
    else:
        accu_reward_methods = script_args.reward_method.split(":")
        assert len(accu_reward_methods) == len(data_files), f"Number of reward methods must match number of data files: {len(accu_reward_methods)} != {len(data_files)}"
    
    
    if len(data_files) != len(image_folders):
        raise ValueError("Number of data files must match number of image folders")
    
    all_data = []
    for data_file, image_folder, accu_reward_method in zip(data_files, image_folders, accu_reward_methods):
        with open(data_file, 'r') as f:
            data=json.load(f)
            for item in data:
                if 'images' in item:
                    if isinstance(item['images'], str):
                        # Store image path instead of loading the image
                        item['image_path'] = [item['images']]
                        del item['images'] # remove the image column so that it can be loaded later
                    elif isinstance(item['image'], list):
                        # if the image is a list, then it is a list of images (for multi-image input)
                        item['image_path'] = item['images']
                        del item['images'] # remove the image column so that it can be loaded later
                    else:
                        raise ValueError(f"Unsupported image type: {type(item['image'])}")
                # Remove immediate image loading
                item['problem'] = item['messages'][0]['content'].replace('<image>', '')
                
                # Handle solution that could be a float or string
                solution_value = item['messages'][1]['content']
                if isinstance(solution_value, str):
                    item['solution'] = solution_value.replace('<answer>', '').replace('</answer>', '').strip()
                else:
                    # If it's a float or other non-string type, keep it as is
                    item['solution'] = str(solution_value)
                item['dataset_name']=script_args.dataset_name
                del item['messages']
                item['accu_reward_method'] = item.get('accu_reward_method', accu_reward_method) # if accu_reward_method is in the data jsonl, use the value in the data jsonl, otherwise use the defined value
                all_data.append(item)

    dataset = Dataset.from_list(all_data)

    def make_conversation_from_jsonl(example):
        if 'image_path' in example and example['image_path'] is not None:
            # Don't load image here, just store the path
            return {
                #'max_score' : example['max_score'],
                'image_path': [p for p in example['image_path']],  # Store path instead of loaded image
                'dataset_name': example['dataset_name'],
                'problem': example['problem'],
                'solution': f"<answer> {example['solution']} </answer>",
                'accu_reward_method': example['accu_reward_method'],
                'prompt': [{
                    'role': 'user',
                    'content': [
                        *({'type': 'image', 'text': None} for _ in range(len(example['image_path']))),
                        {'type': 'text', 'text': example['problem']}
                    ]
                }]
            }
        else:
            return {
                'problem': example['problem'],
                'solution': f"<answer> {example['solution']} </answer>",
                'accu_reward_method': example['accu_reward_method'],
                'prompt': [{
                    'role': 'user',
                    'content': [
                        {'type': 'text', 'text': example['problem']}
                    ]
                }]
            }

    # Map the conversations
    dataset = dataset.map(make_conversation_from_jsonl, num_proc=8)

    # Split dataset for validation if requested
    splits = {'train': dataset}
    if script_args.val_split_ratio > 0:
        train_val_split = dataset.train_test_split(
            test_size=script_args.val_split_ratio
        )
        splits['train'] = train_val_split['train']
        splits['validation'] = train_val_split['test']

    # Select trainer class based on vlm_trainer argument
    trainer_cls = VLMGRPOTrainer
    print("using trainer:", trainer_cls.__name__)
    initialize_tokenizer(model_args.model_name_or_path)

    # Initialize the GRPO trainer
    trainer = trainer_cls(
        model=model_args.model_name_or_path,
        reward_funcs=reward_funcs,
        args=training_args,
        vlm_module=vlm_module_cls(),
        train_dataset=splits['train'],
        eval_dataset=splits.get('validation') if training_args.eval_strategy != "no" else None,
        peft_config=get_peft_config(model_args),
        freeze_vision_modules=model_args.freeze_vision_modules,
        attn_implementation=model_args.attn_implementation,
        max_pixels=script_args.max_pixels,
        min_pixels=script_args.min_pixels,
    )

    # Train and push the model to the Hub
    if list(pathlib.Path(training_args.output_dir).glob("checkpoint-*")):
        trainer.train(resume_from_checkpoint=True)
    else:
        trainer.train()

    # Save and push to hub
    trainer.save_model(training_args.output_dir)
    if training_args.push_to_hub:
        trainer.push_to_hub()


if __name__ == "__main__":
    parser = TrlParser((GRPOScriptArguments, GRPOConfig, GRPOModelConfig))
    script_args, training_args, model_args = parser.parse_args_and_config()
    if training_args.deepspeed and "zero3" in training_args.deepspeed:
        print("zero3 is used, qwen2_5vl forward monkey patch is applied")
        monkey_patch_qwen2_5vl_forward()
    print(training_args.beta)
    
    main(script_args, training_args, model_args)
